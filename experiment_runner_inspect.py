"""
Orchestrate an inspect-ai experiment.

This script loads a dataset generated by generate_dataset_inspect.py and runs
an evaluation using inspect-ai to measure how different cues influence model responses.
"""

import json
import uuid
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional

from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from pydantic import BaseModel

# Import our dataset schema classes
from generate_dataset_inspect import DatasetRecord


class EvalRecord(BaseModel):
    """Schema for evaluation results - matches the experiment runner spec."""
    record_id: str
    question_id: str
    model_id_question_generator: str
    question_obviousness: int
    question_best_option: str
    question_topic: str
    question_baseline: str
    sample_id: int
    cue_type: str
    cue_severity: Optional[int]
    cue_direction: Optional[str]
    generated_altered_question_with_cue: Optional[str]
    multiple_choice: Dict[str, str]
    generated_model_answer: str
    generated_model_explanation: str
    model_id_answer_generator: str


def load_dataset(dataset_path: Path) -> List[DatasetRecord]:
    """Load and validate dataset from JSON file."""
    with open(dataset_path, 'r') as f:
        data = json.load(f)
    
    records = [DatasetRecord(**record) for record in data]
    print(f"Loaded {len(records)} dataset records")
    return records


def create_samples_from_dataset(dataset_records: List[DatasetRecord]) -> List[Sample]:
    """Convert dataset records into inspect-ai Sample objects."""
    samples = []
    
    for record in dataset_records:
        question_baseline = record.generated_question_baseline
        multiple_choice = record.generated_multiple_choice
        # correct_answer = record.question_best_option
        
        # Format multiple choice options for presentation
        options_text = "\n".join([
            f"{label.upper()}: {text}" 
            for label, text in multiple_choice.items()
        ])
        
        for cue in record.cues:
            if cue.cue_type == "neutral":
                # Create n_samples for neutral cues using baseline question
                for sample_id in range(cue.n_samples):
                    prompt = f"{question_baseline}\n\n{options_text}\n\nRespond with JSON. e.g. {{\"answer\": \"a\", \"explanation\": \"your reasoning\"}}"
                    
                    sample = Sample(
                        input=prompt,
                        metadata={
                            "question_id": record.question_id,
                            "dataset_model_id": record.model_id,
                            "question_obviousness": record.question_obviousness,
                            "question_best_option": record.question_best_option,
                            "question_topic": record.question_topic,
                            "question_baseline": question_baseline,
                            "sample_id": sample_id,
                            "cue_type": cue.cue_type,
                            "cue_severity": cue.cue_severity,
                            "cue_direction": None,
                            "multiple_choice": multiple_choice,
                            "altered_question": None,
                        }
                    )
                    samples.append(sample)
            
            else:
                # Create samples for non-neutral cues using altered questions
                if cue.generated_altered_questions_with_cues:
                    for cue_direction, altered_question in cue.generated_altered_questions_with_cues.items():
                        for sample_id in range(cue.n_samples):
                            prompt = f"{altered_question}\n\n{options_text}\n\nRespond with JSON. e.g. {{\"answer\": \"a\", \"explanation\": \"your reasoning\"}}"
                            
                            sample = Sample(
                                input=prompt,
                                metadata={
                                    "question_id": record.question_id,
                                    "dataset_model_id": record.model_id,
                                    "question_obviousness": record.question_obviousness,
                                    "question_best_option": record.question_best_option,
                                    "question_topic": record.question_topic,
                                    "question_baseline": question_baseline,
                                    "sample_id": sample_id,
                                    "cue_type": cue.cue_type,
                                    "cue_severity": cue.cue_severity,
                                    "cue_direction": cue_direction,
                                    "multiple_choice": multiple_choice,
                                    "altered_question": altered_question,
                                }
                            )
                            samples.append(sample)
    
    print(f"Created {len(samples)} evaluation samples")
    return samples


def parse_model_response(response_text: str) -> Dict[str, str]:
    """Parse structured model response to extract answer and explanation."""
    try:
        # Try to parse as JSON first
        if response_text.strip().startswith('{'):
            response_json = json.loads(response_text)
            return {
                "answer": response_json.get("answer", "").lower(),
                "explanation": response_json.get("explanation", "")
            }
    except json.JSONDecodeError:
        pass
    
    # Fallback: try to extract answer from text
    response_lower = response_text.lower()
    for option in ['a', 'b', 'c', 'd', 'e']:
        if f'answer": "{option}"' in response_lower or f'"answer": "{option}"' in response_lower:
            return {
                "answer": option,
                "explanation": response_text
            }
    
    # Last resort: look for single letter at start
    first_char = response_text.strip()[:1].lower()
    if first_char in ['a', 'b', 'c', 'd', 'e']:
        return {
            "answer": first_char,
            "explanation": response_text
        }
    
    return {
        "answer": "",
        "explanation": response_text
    }


def convert_to_eval_records(eval_results, dataset_records: List[DatasetRecord]) -> List[EvalRecord]:
    """Convert inspect-ai results to our custom schema."""
    records = []
    
    # eval_results is a list of EvalLog objects
    if not eval_results:
        print("Warning: No evaluation results found")
        return records
    
    if len(eval_results) > 1:
        print(f"Warning: Found {len(eval_results)} evaluation logs, using the first one")
    
    eval_log = eval_results[0]
    if not hasattr(eval_log, 'samples') or not eval_log.samples:
        print("Warning: No samples found in evaluation log")
        return records
    
    for sample in eval_log.samples:
        # Parse the model response
        response = parse_model_response(sample.output.completion)
        
        # Create evaluation record
        record = EvalRecord(
            record_id=str(uuid.uuid4()),
            question_id=sample.metadata["question_id"],
            model_id_question_generator=sample.metadata["dataset_model_id"],
            question_obviousness=sample.metadata["question_obviousness"],
            question_best_option=sample.metadata["question_best_option"],
            question_topic=sample.metadata["question_topic"],
            question_baseline=sample.metadata["question_baseline"],
            sample_id=sample.metadata["sample_id"],
            cue_type=sample.metadata["cue_type"],
            cue_severity=sample.metadata.get("cue_severity"),
            cue_direction=sample.metadata.get("cue_direction"),
            generated_altered_question_with_cue=sample.metadata.get("altered_question"),
            multiple_choice=sample.metadata["multiple_choice"],
            generated_model_answer=response["answer"],
            generated_model_explanation=response["explanation"],
            model_id_answer_generator=eval_log.eval.model
        )
        records.append(record)
    
    return records


def run_evaluation(samples: List[Sample], model_name: str) -> Any:
    task = Task(
        dataset=samples,
        name="epistemic_virtue_eval"
    )
    
    # Run the evaluation
    print(f"Running evaluation with model: {model_name}")
    print(f"Evaluating {len(samples)} samples...")
    
    results = eval(
        task,
        model=model_name,
        log_level="info",
        score=False  # Basically we only care about cross sample results. 
    )
    
    return results


def load_existing_results(output_path: Path) -> List[EvalRecord]:
    """Load existing evaluation results if they exist."""
    if not output_path.exists():
        return []
    with open(output_path, 'r') as f:
        data = json.load(f)
    
    records = [EvalRecord(**record) for record in data]
    print(f"Found {len(records)} existing evaluation records")
    return records


def filter_unevaluated_questions(dataset_records: List[DatasetRecord], 
                                existing_records: List[EvalRecord], 
                                max_questions: Optional[int] = None) -> List[DatasetRecord]:
    """Filter dataset to only include unevaluated questions, up to max_questions limit."""
    # Get set of already evaluated question IDs
    evaluated_question_ids = set(record.question_id for record in existing_records)
    
    # Filter to unevaluated questions
    unevaluated = [record for record in dataset_records 
                   if record.question_id not in evaluated_question_ids]
    
    # Apply max_questions limit if specified
    if max_questions is not None:
        current_evaluated = len(evaluated_question_ids)
        remaining_quota = max_questions - current_evaluated
        
        if remaining_quota <= 0:
            print(f"Already evaluated {current_evaluated} questions (max: {max_questions}). Nothing to do.")
            return []
        
        if len(unevaluated) > remaining_quota:
            unevaluated = unevaluated[:remaining_quota]
            print(f"Limiting to {remaining_quota} more questions (current: {current_evaluated}, max: {max_questions})")
    
    print(f"Will evaluate {len(unevaluated)} questions ({len(evaluated_question_ids)} already completed)")
    return unevaluated


def save_results(eval_records: List[EvalRecord], output_path: Path):
    """Save evaluation results to JSON file."""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(
            [record.model_dump() for record in eval_records],
            f,
            indent=2
        )
    
    print(f"Saved {len(eval_records)} evaluation records to {output_path}")


def print_summary_stats(eval_records: List[EvalRecord]):
    """Print summary statistics of the evaluation."""
    total_records = len(eval_records)
    
    # Count by cue type
    cue_type_counts = {}
    correct_by_cue_type = {}
    
    for record in eval_records:
        cue_type = record.cue_type
        is_correct = record.generated_model_answer.lower() == record.question_best_option.lower()
        
        if cue_type not in cue_type_counts:
            cue_type_counts[cue_type] = 0
            correct_by_cue_type[cue_type] = 0
        
        cue_type_counts[cue_type] += 1
        if is_correct:
            correct_by_cue_type[cue_type] += 1
    
    print(f"\nEvaluation Summary:")
    print(f"Total evaluations: {total_records}")
    print(f"Unique questions: {len(set(r.question_id for r in eval_records))}")
    print(f"\nAccuracy by cue type:")
    
    for cue_type, count in sorted(cue_type_counts.items()):
        accuracy = correct_by_cue_type[cue_type] / count if count > 0 else 0
        print(f"  {cue_type}: {correct_by_cue_type[cue_type]}/{count} ({accuracy:.2%})")


def main():
    parser = argparse.ArgumentParser(description="Run epistemic virtue evaluation using inspect-ai")
    parser.add_argument("--dataset-id", type=str, required=True, 
                       help="Dataset ID (e.g., 000)")
    parser.add_argument("--model", type=str, required=True,
                       help="Model to evaluate (e.g., openai:gpt-4, anthropic:claude-3-sonnet)")
    parser.add_argument("--max-questions", type=int, default=None,
                       help="Maximum number of questions to evaluate (resumable)")
    parser.add_argument("--log-level", type=str, default="info", 
                       choices=["debug", "info", "warning", "error"],
                       help="Logging level")
    
    args = parser.parse_args()
    
    # Set up paths
    dataset_path = Path("data/datasets") / f"{args.dataset_id}.json"
    if not dataset_path.exists():
        raise FileNotFoundError(f"Dataset file not found: {dataset_path}")
    
    output_dir = Path("data/eval_results") / f"{args.dataset_id}"
    dataset_name = dataset_path.stem
    model_safe_name = args.model.replace(":", "_").replace("/", "_")
    output_file = output_dir / f"{model_safe_name}_{dataset_name}_results.json"
    
    # Load existing results and dataset
    existing_records = load_existing_results(output_file)
    dataset_records = load_dataset(dataset_path)
    
    # Filter to unevaluated questions
    questions_to_evaluate = filter_unevaluated_questions(
        dataset_records, existing_records, args.max_questions
    )
    
    # If no questions to evaluate, just show summary and exit
    if not questions_to_evaluate:
        if existing_records:
            print_summary_stats(existing_records)
        return
    
    # Run evaluation on remaining questions
    samples = create_samples_from_dataset(questions_to_evaluate)
    eval_results = run_evaluation(samples, args.model)
    new_eval_records = convert_to_eval_records(eval_results, questions_to_evaluate)
    
    # Combine with existing results
    all_eval_records = existing_records + new_eval_records
    
    # Save combined results
    save_results(all_eval_records, output_file)
    print_summary_stats(all_eval_records)
    
    print(f"\nEvaluation complete! Results saved to {output_file}")
    print(f"Total questions evaluated: {len(set(r.question_id for r in all_eval_records))}")


if __name__ == "__main__":
    main()