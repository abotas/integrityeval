"""
Orchestrate an inspect-ai experiment.

This script loads a dataset generated by generate_dataset_inspect.py and runs
an evaluation using inspect-ai to measure how different cues influence model responses.
"""

import json
import uuid
import argparse
from pathlib import Path
from typing import List, Dict, Optional

from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.log import EvalLog
from pydantic import BaseModel

from generate_dataset import DatasetRecord, parse_json_from_response_text


class EvalRecord(BaseModel):
    """Schema for evaluation results - matches the experiment runner spec."""
    record_id: str
    question_id: str
    model_id_question_generator: str
    question_obviousness: int
    question_best_option: str
    question_topic: str
    question_baseline: str
    sample_id: int
    cue_type: str
    cue_severity: Optional[int]
    cue_direction: Optional[str]
    generated_altered_question_with_cue: Optional[str]
    multiple_choice: Dict[str, str]
    generated_model_answer: str
    generated_model_explanation: str
    model_id_answer_generator: str


def load_dataset(dataset_path: Path) -> List[DatasetRecord]:
    """Load and validate dataset from JSON file."""
    with open(dataset_path, 'r') as f:
        data = json.load(f)
    
    records = [DatasetRecord(**record) for record in data]
    print(f"Loaded {len(records)} dataset records")
    return records


def create_samples_from_dataset(dataset_records: List[DatasetRecord]) -> List[Sample]:
    """Convert dataset records into inspect-ai Sample objects."""
    samples = []
    
    for record in dataset_records:
        question_baseline = record.generated_question_baseline
        multiple_choice = record.generated_multiple_choice
        options_text = "\n".join([
            f"{label.upper()}: {text}" 
            for label, text in multiple_choice.items()
        ])
        prompt_suffix = f"{options_text}\n\nRespond with JSON. e.g. {{\"answer\": \"a\", \"explanation\": \"your reasoning\"}}"
        for cue in record.cues:
            if cue.cue_type == "neutral":
                for sample_id in range(cue.n_samples):
                    samples.append(Sample(
                        input=f"{question_baseline}\n\n{prompt_suffix}",
                        metadata={
                            "question_id": record.question_id,
                            "dataset_model_id": record.model_id,
                            "question_obviousness": record.question_obviousness,
                            "question_best_option": record.question_best_option,
                            "question_topic": record.question_topic,
                            "question_baseline": question_baseline,
                            "sample_id": sample_id,
                            "cue_type": cue.cue_type,
                            "cue_severity": cue.cue_severity,
                            "cue_direction": None,
                            "multiple_choice": multiple_choice,
                            "altered_question": None,
                        }
                    ))
            else:
                for cue_direction, altered_question in cue.generated_altered_questions_with_cues.items():
                    for sample_id in range(cue.n_samples):
                        samples.append(Sample(
                            input=f"{altered_question}\n\n{prompt_suffix}",
                            metadata={
                                "question_id": record.question_id,
                                "dataset_model_id": record.model_id,
                                "question_obviousness": record.question_obviousness,
                                "question_best_option": record.question_best_option,
                                "question_topic": record.question_topic,
                                "question_baseline": question_baseline,
                                "sample_id": sample_id,
                                "cue_type": cue.cue_type,
                                "cue_severity": cue.cue_severity,
                                "cue_direction": cue_direction,
                                "multiple_choice": multiple_choice,
                                "altered_question": altered_question,
                            }
                        ))
    
    print(f"Created {len(samples)} evaluation samples")
    return samples


def parse_model_response(response_text: str) -> Dict[str, str]:
    """Parse structured model response to extract answer and explanation."""
    response_json = json.loads(parse_json_from_response_text(response_text))
    if "answer" not in response_json or "explanation" not in response_json:
        raise ValueError(f"Response missing 'answer' or 'explanation' field: {response_json}")
    answer = response_json["answer"].lower()
    if answer not in ['a', 'b', 'c', 'd', 'e']:
        raise ValueError(f"Invalid answer '{answer}' - must be a, b, c, d, or e")
    return {
        "answer": answer,
        "explanation": response_json["explanation"]
    }


def convert_to_eval_records(eval_log: EvalLog) -> EvalRecord:
    """Convert inspect-ai results to our custom schema."""
    records = []
    for sample in eval_log.samples:
        response = parse_model_response(sample.output.completion)        
        record = EvalRecord(
            record_id=str(uuid.uuid4()),
            question_id=sample.metadata["question_id"],
            model_id_question_generator=sample.metadata["dataset_model_id"],
            question_obviousness=sample.metadata["question_obviousness"],
            question_best_option=sample.metadata["question_best_option"],
            question_topic=sample.metadata["question_topic"],
            question_baseline=sample.metadata["question_baseline"],
            sample_id=sample.metadata["sample_id"],
            cue_type=sample.metadata["cue_type"],
            cue_severity=sample.metadata.get("cue_severity"),
            cue_direction=sample.metadata.get("cue_direction"),
            generated_altered_question_with_cue=sample.metadata.get("altered_question"),
            multiple_choice=sample.metadata["multiple_choice"],
            generated_model_answer=response["answer"],
            generated_model_explanation=response["explanation"],
            model_id_answer_generator=eval_log.eval.model
        )
        records.append(record)
    return records


def run_eval(samples: List[Sample], model_name: str) -> EvalLog:
    task = Task(
        dataset=samples,
        name="epistemic_virtue_eval"
    )
    print(f"Evaling model: {model_name} over{len(samples)} samples.")
    results = eval(
        task,
        model=model_name,
        log_level="info",
        score=False  # We only care about cross sample results. 
    )
    if not len(results) == 1:
        raise ValueError(f"Expected 1 evaluation log, got {len(results)}")
    return results[0]


def load_existing_results(output_path: Path) -> List[EvalRecord]:
    """Load existing evaluation results if they exist."""
    if not output_path.exists():
        return []
    with open(output_path, 'r') as f:
        data = json.load(f)
    
    records = [EvalRecord(**record) for record in data]
    print(f"Found {len(records)} existing evaluation records")
    return records


def filter_unevaluated_questions(dataset_records: List[DatasetRecord], 
                                existing_records: List[EvalRecord], 
                                max_questions: Optional[int] = None) -> List[DatasetRecord]:
    """Filter dataset to only include unevaluated questions, up to max_questions limit."""
    # Get set of already evaluated question IDs
    evaluated_question_ids = set(record.question_id for record in existing_records)
    
    # Filter to unevaluated questions
    unevaluated = [record for record in dataset_records 
                   if record.question_id not in evaluated_question_ids]
    
    # Apply max_questions limit if specified
    if max_questions is not None:
        current_evaluated = len(evaluated_question_ids)
        remaining_quota = max_questions - current_evaluated
        
        if remaining_quota <= 0:
            print(f"Already evaluated {current_evaluated} questions (max: {max_questions}). Nothing to do.")
            return []
        
        if len(unevaluated) > remaining_quota:
            unevaluated = unevaluated[:remaining_quota]
            print(f"Limiting to {remaining_quota} more questions (current: {current_evaluated}, max: {max_questions})")
    
    print(f"Will evaluate {len(unevaluated)} questions ({len(evaluated_question_ids)} already completed)")
    return unevaluated


def save_results(eval_records: List[EvalRecord], output_path: Path):
    """Save evaluation results to JSON file."""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(
            [record.model_dump() for record in eval_records],
            f,
            indent=2
        )
    
    print(f"Saved {len(eval_records)} evaluation records to {output_path}")

def main():
    parser = argparse.ArgumentParser(description="Run epistemic virtue evaluation using inspect-ai")
    parser.add_argument("--dataset-id", type=str, required=True, 
                       help="Dataset ID (e.g., 000)")
    parser.add_argument("--model", type=str, required=True,
                       help="Model to evaluate (e.g., google/gemini-2.5-flash)")
    parser.add_argument("--max-questions", type=int, default=None,
                       help="Maximum number of questions to evaluate (resumable)")
    
    args = parser.parse_args()
    dataset_path = Path("data/datasets") / f"{args.dataset_id}.json"
    if not dataset_path.exists():
        raise FileNotFoundError(f"Dataset file not found: {dataset_path}")
    output_dir = Path("data/eval_results") / f"{args.dataset_id}"
    dataset_name = dataset_path.stem
    model_safe_name = args.model.replace(":", "_").replace("/", "_")
    output_file = output_dir / f"{model_safe_name}_{dataset_name}_results.json"
    
    existing_records = load_existing_results(output_file)
    dataset_records = load_dataset(dataset_path)
    questions_to_evaluate = filter_unevaluated_questions(
        dataset_records, existing_records, args.max_questions
    )
    if not questions_to_evaluate:  # Already evaluated all questions.
        return
    
    samples = create_samples_from_dataset(questions_to_evaluate)
    eval_log = run_eval(samples, args.model)
    all_eval_records = existing_records + convert_to_eval_records(eval_log)
    save_results(all_eval_records, output_file)
    print(f"\nEvaluation complete! Results saved to {output_file}")
    print(f"Total questions evaluated: {len(set(r.question_id for r in all_eval_records))}")


if __name__ == "__main__":
    main()