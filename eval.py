"""
Orchestrate an inspect-ai experiment.

This script loads a dataset generated by generate_dataset_inspect.py and runs
an evaluation using inspect-ai to measure how different cues influence model responses.
"""

import json
import uuid
import argparse
from pathlib import Path
from typing import List, Dict, Optional, Literal

import json_repair
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.log import EvalLog
from pydantic import BaseModel, field_validator

from generate_dataset import DatasetRecord, parse_json_from_response_text
from prompt_templates import EVAL_SAMPLE_SUFFIX_TEMPLATE, EVAL_CTRL_TEMPLATE


class ModelResponse(BaseModel):
    """Schema for main model response."""
    answer: Literal['a', 'b', 'c', 'd', 'e']
    explanation: str
    
    @field_validator('answer', mode='before')
    def lowercase_answer(cls, v):
        return v.lower() if isinstance(v, str) else v


class UnfairnessResponse(BaseModel):
    """Schema for unfairness check response."""
    yes_or_no: Literal['yes', 'no']
    explanation: str
    
    @field_validator('yes_or_no', mode='before')
    def lowercase_yes_or_no(cls, v):
        return v.lower() if isinstance(v, str) else v


class EvalRecord(BaseModel):
    """Schema for evaluation results - matches the experiment runner spec."""
    record_id: str
    question_id: str
    model_id_question_generator: str
    question_obviousness: int
    question_best_option: str
    question_topic: str
    question_baseline: str
    sample_id: int
    cue_id: str
    cue_type: str
    cue_severity: Optional[int]
    cue_direction: Optional[str]
    generated_context_with_cue: Optional[str]
    multiple_choice: Dict[str, str]
    generated_model_answer: str
    generated_model_explanation: str
    generated_ctrl_unfairness: Optional[bool]
    generated_ctrl_unfairness_explanation: Optional[str]
    model_id_answer_generator: str

def load_dataset(dataset_path: Path) -> List[DatasetRecord]:
    """Load and validate dataset from JSON file."""
    with open(dataset_path, 'r') as f:
        data = json.load(f)
    
    records = [DatasetRecord(**record) for record in data]
    print(f"Loaded {len(records)} dataset records")
    return records

def create_samples_from_dataset(dataset_records: List[DatasetRecord], human_approved_only: bool = False) -> List[Sample]:
    """Convert dataset records into inspect-ai Sample objects."""
    samples = []
    
    for record in dataset_records:
        question_baseline = record.generated_question_baseline
        multiple_choice = record.generated_multiple_choice
        options_text = "\n".join([
            f"{label.upper()}: {text}" 
            for label, text in multiple_choice.items()
        ])
        prompt_suffix = EVAL_SAMPLE_SUFFIX_TEMPLATE.format(
            question_baseline=question_baseline,
            options_text=options_text
        )
        for cue in record.cues:
            # Filter out non-human-approved cues if requested
            if human_approved_only and cue.cue_type != "neutral" and cue.human_approved is not True:
                continue
                
            if cue.cue_type == "neutral":
                for sample_id in range(cue.n_samples):
                    samples.append(Sample(
                        input=prompt_suffix,
                        metadata={
                            "question_id": record.question_id,
                            "dataset_model_id": record.model_id,
                            "question_obviousness": record.question_obviousness,
                            "question_best_option": record.question_best_option,
                            "question_topic": record.question_topic,
                            "question_baseline": question_baseline,
                            "sample_id": sample_id,
                            "cue_id": cue.cue_id,
                            "cue_type": cue.cue_type,
                            "cue_severity": cue.cue_severity,
                            "cue_direction": None,
                            "multiple_choice": multiple_choice,
                            "context_with_cue": None,
                            "sample_type": "main",
                        }
                    ))
            else:
                for cue_direction, context_with_cue in cue.generated_context_with_cues.items():
                    # Create unfairness check sample for this cue
                    unfairness_prompt = EVAL_CTRL_TEMPLATE.format(
                        context=context_with_cue,
                        question=question_baseline,
                        options_text=options_text,
                        question_baseline=question_baseline
                    )
                    samples.append(Sample(
                        input=unfairness_prompt,
                        metadata={
                            "question_id": record.question_id,
                            "dataset_model_id": record.model_id,
                            "cue_id": cue.cue_id,
                            "cue_type": cue.cue_type,
                            "cue_severity": cue.cue_severity,
                            "cue_direction": cue_direction,
                            "context_with_cue": context_with_cue,
                            "sample_type": "unfairness_check",
                        }
                    ))
                    
                    # Create main evaluation samples
                    for sample_id in range(cue.n_samples):
                        # Append the original question to the context
                        samples.append(Sample(
                            input=f"{context_with_cue}\n\n{prompt_suffix}",
                            metadata={
                                "question_id": record.question_id,
                                "dataset_model_id": record.model_id,
                                "question_obviousness": record.question_obviousness,
                                "question_best_option": record.question_best_option,
                                "question_topic": record.question_topic,
                                "question_baseline": question_baseline,
                                "sample_id": sample_id,
                                "cue_id": cue.cue_id,
                                "cue_type": cue.cue_type,
                                "cue_severity": cue.cue_severity,
                                "cue_direction": cue_direction,
                                "multiple_choice": multiple_choice,
                                "context_with_cue": context_with_cue,
                                "sample_type": "main",
                            }
                        ))
    
    print(f"Created {len(samples)} evaluation samples")
    return samples

def parse_model_response(response_text: str) -> ModelResponse:
    """Parse structured model response to extract answer and explanation."""
    try:
        response_json = json.loads(parse_json_from_response_text(response_text))
    except json.JSONDecodeError:
        response_json = json_repair.loads(response_text)
    
    return ModelResponse(**response_json)

def parse_unfairness_response(response_text: str) -> UnfairnessResponse:
    """Parse unfairness check response to extract yes/no and explanation."""
    try:
        response_json = json.loads(parse_json_from_response_text(response_text))
    except json.JSONDecodeError:
        response_json = json_repair.loads(response_text)
    
    return UnfairnessResponse(**response_json)

def convert_to_eval_records(eval_log: EvalLog) -> List[EvalRecord]:
    """Convert inspect-ai results to our custom schema."""
    # First, build unfairness lookup: (question_id, cue_type, cue_severity, cue_direction) -> unfairness result
    unfairness_lookup = {}
    for sample in eval_log.samples:
        if sample.metadata.get("sample_type") == "unfairness_check":
            try: 
                response = parse_unfairness_response(sample.output.completion)
                unfair = False if response.yes_or_no == "no" else True
                unfairness_explanation = response.explanation
            except Exception as e:
                print(f"Error parsing unfairness response: {e}")
                print(f"Response text: {sample.output.completion}")
                unfair = True
                unfairness_explanation = f"By default. Could not parse model response: {sample.output.completion}"

            
            key = (
                sample.metadata["question_id"],
                sample.metadata["cue_id"],
                sample.metadata["cue_type"],
                sample.metadata["cue_severity"],
                sample.metadata["cue_direction"]
            )
            unfairness_lookup[key] = (unfair, unfairness_explanation)
    
    # Now process main samples
    records = []
    for sample in eval_log.samples:
        if sample.metadata.get("sample_type") == "main":
            try:
                response = parse_model_response(sample.output.completion)
                answer = response.answer
                explanation = response.explanation
            except (ValueError, KeyError, json.JSONDecodeError, TypeError):
                # TODO: Actually ensure it's a refusal rather than a failed response.
                answer = "REFUSED"
                explanation = f"Model output: {sample.output.completion[:500]}..."
            
            # Look up fairness result for non-neutral cues
            if sample.metadata["cue_type"] != "neutral":
                key = (
                    sample.metadata["question_id"],
                    sample.metadata["cue_id"],
                    sample.metadata["cue_type"],
                    sample.metadata["cue_severity"],
                    sample.metadata["cue_direction"]
                )
                unfairness_result = unfairness_lookup.get(key)
                if unfairness_result:
                    unfair, unfairness_explanation = unfairness_result
                else:
                    unfair = None
                    unfairness_explanation = None
            else:
                # Neutral cues don't need fairness checks
                unfair = None
                unfairness_explanation = None
            
            record = EvalRecord(
                record_id=str(uuid.uuid4()),
                question_id=sample.metadata["question_id"],
                model_id_question_generator=sample.metadata["dataset_model_id"],
                question_obviousness=sample.metadata["question_obviousness"],
                question_best_option=sample.metadata["question_best_option"],
                question_topic=sample.metadata["question_topic"],
                question_baseline=sample.metadata["question_baseline"],
                sample_id=sample.metadata["sample_id"],
                cue_id=sample.metadata["cue_id"],
                cue_type=sample.metadata["cue_type"],
                cue_severity=sample.metadata.get("cue_severity"),
                cue_direction=sample.metadata.get("cue_direction"),
                generated_context_with_cue=sample.metadata.get("context_with_cue"),
                multiple_choice=sample.metadata["multiple_choice"],
                generated_model_answer=answer,
                generated_model_explanation=explanation,
                generated_ctrl_unfairness=unfair,
                generated_ctrl_unfairness_explanation=unfairness_explanation,
                model_id_answer_generator=eval_log.eval.model
            )
            records.append(record)
    return records

def run_eval(samples: List[Sample], model_name: str) -> EvalLog:
    task = Task(
        dataset=samples,
        name="epistemic_virtue_eval"
    )
    print(f"Evaling model: {model_name} over{len(samples)} samples.")
    results = eval(
        task,
        model=model_name,
        log_level="info",
        score=False,  # We only care about cross sample results. 
        max_retries=2
    )
    if not len(results) == 1:
        raise ValueError(f"Expected 1 evaluation log, got {len(results)}")
    return results[0]

def load_existing_results(output_path: Path) -> List[EvalRecord]:
    """Load existing evaluation results if they exist."""
    if not output_path.exists():
        return []
    with open(output_path, 'r') as f:
        data = json.load(f)
    
    records = [EvalRecord(**record) for record in data]
    print(f"Found {len(records)} existing evaluation records")
    return records

def filter_unevaluated_questions(dataset_records: List[DatasetRecord], 
                                existing_records: List[EvalRecord], 
                                max_questions: Optional[int] = None) -> List[DatasetRecord]:
    """Filter dataset to only include unevaluated questions, up to max_questions limit."""
    # Get set of already evaluated question IDs
    evaluated_question_ids = set(record.question_id for record in existing_records)
    
    # Filter to unevaluated questions
    unevaluated = [record for record in dataset_records 
                   if record.question_id not in evaluated_question_ids]
    
    # Apply max_questions limit if specified
    if max_questions is not None:
        current_evaluated = len(evaluated_question_ids)
        remaining_quota = max_questions - current_evaluated
        
        if remaining_quota <= 0:
            print(f"Already evaluated {current_evaluated} questions (max: {max_questions}). Nothing to do.")
            return []
        
        if len(unevaluated) > remaining_quota:
            unevaluated = unevaluated[:remaining_quota]
            print(f"Limiting to {remaining_quota} more questions (current: {current_evaluated}, max: {max_questions})")
    
    print(f"Will evaluate {len(unevaluated)} questions ({len(evaluated_question_ids)} already completed)")
    return unevaluated

def save_results(eval_records: List[EvalRecord], output_path: Path):
    """Save evaluation results to JSON file."""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump([record.model_dump() for record in eval_records], f, indent=2)
    
    print(f"Saved {len(eval_records)} evaluation records to {output_path}")

def main():
    parser = argparse.ArgumentParser(description="Run epistemic virtue evaluation using inspect-ai")
    parser.add_argument("--dataset-id", type=str, required=True, 
                       help="Dataset ID (e.g., 000)")
    parser.add_argument("--model", type=str, required=True,
                       help="Model to evaluate (e.g., google/gemini-2.5-flash)")
    parser.add_argument("--max-questions", type=int, default=None,
                       help="Maximum number of questions to evaluate (resumable)")
    parser.add_argument("--human-approved-only", action="store_true",
                       help="Only evaluate human-approved cues")
    
    args = parser.parse_args()
    dataset_path = Path("data/datasets") / f"{args.dataset_id}.json"
    if not dataset_path.exists():
        raise FileNotFoundError(f"Dataset file not found: {dataset_path}")
    output_dir = Path("data/eval_results") / f"{args.dataset_id}"
    dataset_name = dataset_path.stem
    model_safe_name = args.model.replace(":", "_").replace("/", "_")
    output_file = output_dir / f"{model_safe_name}_{dataset_name}_results.json"
    
    existing_records = load_existing_results(output_file)
    dataset_records = load_dataset(dataset_path)
    questions_to_evaluate = filter_unevaluated_questions(
        dataset_records, existing_records, args.max_questions
    )
    if not questions_to_evaluate:  # Already evaluated all questions.
        return
    
    samples = create_samples_from_dataset(questions_to_evaluate, args.human_approved_only)
    eval_log = run_eval(samples, args.model)
    all_eval_records = existing_records + convert_to_eval_records(eval_log)
    save_results(all_eval_records, output_file)
    print(f"\nEvaluation complete! Results saved to {output_file}")
    print(f"Total questions evaluated: {len(set(r.question_id for r in all_eval_records))}")


if __name__ == "__main__":
    main()